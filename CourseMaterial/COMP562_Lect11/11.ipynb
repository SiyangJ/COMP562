{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 â€“ Lecture 11  \n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\EE}{\\mathbb{E}}\n",
    "\\renewcommand{\\KL}{\\textrm{KL}}\n",
    "\\renewcommand{\\Bound}{\\mathcal{B}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "\\renewcommand{\\new}{\\textrm{new}}\n",
    "\\renewcommand{\\old}{\\textrm{old}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples of Unsupervised Learning\n",
    "\n",
    "* Dimensionality reduction -- lossy compression \n",
    "* Clustering -- assigning each point to a representative cluster\n",
    "  * Note: in classification groups are known, in clustering they are learned\n",
    "* Deconvolution -- splitting mixed signals such as instruments or speakers in sound signal\n",
    "\n",
    "Applications\n",
    "* Data summarization/compression\n",
    "* Denoising, outlier detection\n",
    "* Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "We will first look at a very simple -- and popular -- clustering algorithm called **K-means**\n",
    "\n",
    "* Randomly choose K cluster center locations (means or centroids)\n",
    "* Loop until convergence\n",
    "    1. Assignment of samples to the closest of the K centroids\n",
    "    2. Re-estimate the cluster\tcentroids or means based on\tthe\tdata assigned to each cluster\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-means Observations\n",
    "\n",
    "1. Initialization is random -- means of the clusters are slightly preturbed versions of data mean\n",
    "2. Clusters are assumed to be spherical\n",
    "3. Must manually choose K\n",
    "3. Clusters can have zero members -- make sure there is no division with zero \n",
    "\n",
    "```\n",
    "mus = numpy.dot(xs,ph.transpose())/(1e-5 + numpy.sum(ph,axis=1))\n",
    "```\n",
    "4. There are local minima\n",
    "   * Non-trivial: poor initialization, too small or too large K\n",
    "5. Multiple restarts -- from different initializations -- can lead to better solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we Come up with an Algorithm such as K-means?\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Write out a generative model for the data\n",
    "2. Write out log-likelihood\n",
    "3. Optimize log-likelihood\n",
    "\n",
    "The twist compared to our previous model learning is in the fact that not all the variables are observed\n",
    "\n",
    "Labels, indicating cluster membership, are **hidden** from us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mixture Models -- Generative Story\n",
    "\n",
    "Each sample $\\xx$ is generated by:\n",
    "1. Selecting a cluster, according to some distribution $\\pi = (\\pi_1,...,\\pi_K)$\n",
    "$$\n",
    "p(h) = \\pi_h\n",
    "$$\n",
    "2. Using parameters of cluster $h$ generate the sample $\\xx$\n",
    "$$\n",
    "p(\\xx \\mid h,\\theta) = p(\\xx \\mid \\theta_h)\n",
    "$$\n",
    "\n",
    "For example in **K-means**, you can think of $\\theta_h$ as the mean of the cluster $h$\n",
    "\n",
    "$$\n",
    "p(\\xx \\mid \\theta_h) = \\prod_{j=1}^p \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\myexp{-\\frac{1}{2\\sigma^2}(x_j - \\theta_{h,j})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mixture Models -- Generative Story\n",
    "\n",
    "Note that generative model talks about how data might have been generated\n",
    "\n",
    "We want to fit a model under which that data is most probable\n",
    "\n",
    "To do so, we must write out log-likelihood \n",
    "$$\n",
    "\\loglik(\\Theta) = \\sum_i \\log p(\\xx_i \\mid\\Theta)\n",
    "$$\n",
    "and maximize it with respect to parameters $\\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Marginal Log-Likelihood \n",
    "\n",
    "For a single sample $\\xx$, we know how to compute $p(h)$ and $p(\\xx\\mid h,\\Theta)$ so we can obtain joint \n",
    "\n",
    "$$\n",
    "p(\\xx,h \\mid \\Theta) = p(h) p(\\xx\\mid h,\\Theta)\n",
    "$$\n",
    "\n",
    "This is probability of the full configuration $\\xx$ **and** cluster membership $h$\n",
    "\n",
    "Our data does not contain information about cluster membership, just vectors $\\xx$\n",
    "\n",
    "**<font color='red'> How do we compute $p(\\xx \\mid \\Theta)$?  </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Marginal Log-likelihood \n",
    "\n",
    "We can use the fact that\n",
    "$$\n",
    "p(\\xx\\mid\\Theta) = \\sum_h p(\\xx,h \\mid \\Theta)  = \\sum_h p(h)p(\\xx\\mid h,\\Theta).\n",
    "$$\n",
    "\n",
    "**<font color='red'>  What is the interpretation of this sum? </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Marginal Log-likelihood\n",
    "Now, we can express log-likelihood in terms of probabilities in our model\n",
    "\n",
    "$$\n",
    "\\loglik(\\Theta) = \\sum_i \\log p(\\xx_i\\mid\\Theta) = \\sum_{i=1}^N \\log  \\sum_{h_i} p(\\xx_i,h_i\\mid \\Theta).\n",
    "$$\n",
    "\n",
    "Observations\n",
    "1. For each sample $\\xx_i$, we have corresponding cluster membership variable $h_i$\n",
    "2. There is a sum under the log so we cannot push the log to probability terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with Difficult Objectives\n",
    "\n",
    "Typically, when we run into an objective that is difficult to work with (for example log of sums above), we seek a closely related objective that is easier\n",
    "\n",
    "Hence, we will not directly optimize the log-likelihood instead we will introduce a lower-bound on the log-likelihood which we can show is tight at the optimum\n",
    "\n",
    "To accomplish this we a bit of math that you may not be familar with\n",
    "\n",
    "1. Convex and concave functions\n",
    "2. Jensen's inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concave and Convex Functions \n",
    "\n",
    "A function $f(x)$ is concave if\n",
    "\n",
    "$$\n",
    "f(\\lambda x + (1-\\lambda)y) \\geq \\lambda f(x)  + (1-\\lambda)f(y)\n",
    "$$\n",
    "\n",
    "for any $\\lambda \\in [0,1]$\n",
    "\n",
    "Examples of concave functions are log, exp, square, etc.\n",
    "\n",
    "Similarly, a function $f(x)$ is convex if\n",
    "\n",
    "$$\n",
    "f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x)  + (1-\\lambda)f(y)\n",
    "$$\n",
    "\n",
    "for any $\\lambda \\in [0,1]$\n",
    "\n",
    "The negative of a concave function is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jensen's Inequality\n",
    "\n",
    "Jensen's inequality\n",
    "\n",
    "$$\n",
    "f(\\EE[H]) \\geq \\EE[f(H)]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\EE[H] = \\sum_h p(H=h)h.\n",
    "$$ \n",
    "\n",
    "and $f$ is concave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bounding log-Likelihood\n",
    "\n",
    "Starting with log-likelihood\n",
    "$$\n",
    "\\loglik(\\Theta) = \\sum_i \\log p(\\xx_i\\mid\\Theta) = \\sum_{i=1}^N \\log  \\sum_{h_i} p(\\xx_i,h_i\\mid \\Theta).\n",
    "$$\n",
    "we introduce distributions $q_i(h_i)$ and multiply and divide by them\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\loglik(\\Theta) &= \\sum_i \\log p(\\xx_i\\mid\\Theta) = \\sum_{i=1}^N \\log  \\sum_{h_i} \\color{red}{\\frac{q_i(h_i)}{q_i(h_i)}} p(\\xx_i,h_i\\mid \\Theta)\\\\\n",
    "&= \\sum_{i=1}^N \\log  \\sum_{h_i} q_t(h_i) \\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) } \\\\\n",
    "&= \\sum_{i=1}^N \\log \\EE_{q_i}\\left[\\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) }\\right] \\\\\n",
    "&\\geq \\sum_{i=1}^N \\EE_{q_i}\\left[\\log \\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) }\\right] \\\\\n",
    "&= \\sum_{i=1}^N \\sum_{h_i} q_i(h_i) \\log \\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) } \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bounding log-Likelihood\n",
    "\n",
    "Starting with log-likelihood\n",
    "$$\n",
    "\\loglik(\\Theta) = \\sum_i \\log p(\\xx_i\\mid\\Theta) \\geq  \\sum_{i=1}^N \\sum_{h_i} q_i(h_i) \\log \\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) }  = \\mathcal{B}(\\Theta,q)\n",
    "$$\n",
    "\n",
    "Natural questions that arise:\n",
    "1. Where do we get $q_i$s? \n",
    "2. Does optimizing bound result in the same $\\Theta$?\n",
    "$$\n",
    "\\argmax_{\\Theta}\\loglik(\\Theta)\\stackrel{?}{=}\\argmax_{\\Theta}\\mathcal{B}(\\Theta,q) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bounding log-Likelihood\n",
    "\n",
    "Natural questions that arise:\n",
    "\n",
    "*  Where do we get $q_i$s? \n",
    " * A: We use posterior probabilities $p(h_i \\mid \\xx_i,\\Theta)$\n",
    "\n",
    "\n",
    "*  $\\argmax_{\\Theta} \\loglik(\\Theta) \\stackrel{?}{=} \\argmax_{\\Theta} \\mathcal{B}(\\Theta,q)$? \n",
    " * A: Yes, if we use exact posterior probabilities in place of $q_i$s the two objectives coincide and optimal $\\Theta$s are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Hence we can maximize the bound $\\mathcal{B}(\\Theta)$ by iterating\n",
    "\n",
    "1. (E-step) Computing the optimal \n",
    "$$\n",
    "q^{\\new}_i = \\argmax_{q_i} \\mathcal{B}(\\Theta^{\\old},q) \n",
    "$$\n",
    "2. (M-step) Updating $\\Theta$ given current $q_i(h_i)$\n",
    "$$\n",
    "\\Theta^{\\new} = \\argmax_{\\Theta} \\mathcal{B}(\\Theta,q^{\\new}) \n",
    "$$\n",
    "\n",
    "Recall for a moment the K-means algorithm. It alternated analogous two steps:\n",
    "1. Assigning each sample to a cluster\n",
    "2. Cluster center computation based on assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EM Algorithm for Mixture of Gaussians with Spherical Covariance\n",
    "\n",
    "The model\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(h\\mid \\alpha) &= \\alpha_h \\\\\n",
    "p(\\xx \\mid h,\\mu) &= (2\\pi)^{-\\frac{d}{2}} \\myexp{-\\frac{1}{2}(\\xx - \\mu_{h_t})^T(\\xx - \\mu_{h_t})} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a variant of **Mixture of Gaussians**\n",
    "\n",
    "$\\alpha_c$ is an a-priori probability that a sample comes from class $c$ -- also called **mixing proportion**\n",
    "\n",
    "The bound \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Bound(\\Theta,q) &= \\sum_{i=1}^N \\sum_{h_i} q_i(h_i) \\log \\frac{ p(\\xx_i,h_i\\mid \\Theta) }{ q_i(h_i) } \\\\\n",
    "&=  \\sum_{i=1}^T \\sum_{h_i} q_i(h_i) \\left[ \\log \\alpha_{h_i} -\\frac{d}{2} \\log (2\\pi) -\\frac{1}{2}(\\xx - \\mu_{h_i})^T(\\xx - \\mu_{h_i}) \\right] \\\\\n",
    "&- \\sum_{i=1}^T \\sum_{h_i} q_i(h_i) \\log q_i(h_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "In this case $\\Theta = (\\alpha_1,...,\\alpha_K,\\mu_1, ...,\\mu_K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# E-step\n",
    "\n",
    "The E-step\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_i(h_i = k) &= p(h_i =k \\mid \\xx_i, \\mu)  = \\frac{p(\\xx_i,h_i = k \\mid \\mu)}{\\underbrace{\\sum_c p(\\xx_i,h_i=c \\mid \\mu)}_{\\textrm{same for all values of } k}}\\\\\n",
    " &\\propto p(\\xx_i,h_i = k\\mid \\mu)\\\\\n",
    "        &=  \\alpha_{h_i} (2\\pi)^{-\\frac{d}{2}} \\myexp{-\\frac{1}{2}(\\xx - \\mu_h)^T(\\xx - \\mu_h)}\n",
    "\\end{aligned}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# E-step: Working in Log-Domain\n",
    "\n",
    "If the data vectors are long, the computation of joint probabilities can yield very tiny probabilities\n",
    "\n",
    "Rather than working with probabilities we work with log-probabilities. Hence we store \n",
    "\n",
    "$$\n",
    "\\log q_i(h_i = k) = \\log p(\\xx_i,h_i = k \\mid \\mu) - \\log \\sum_c p(\\xx_i,h_i=c \\mid \\mu)\n",
    "$$\n",
    "\n",
    "If all the probabilities are stored in log-domain, then computation of their sum requires exponentation\n",
    "\n",
    "$$\n",
    "\\log \\sum_c p(\\xx_i,h_i=c \\mid \\mu) = \\log \\sum_c\\exp \\ \\underbrace{\\log p(\\xx_i,h_i=c \\mid \\mu)}_{\\textrm{stored log-probability}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# M-step\n",
    "\n",
    "In M-step we optimize $\\Theta$ given $q$s\n",
    "$$\n",
    "\\Theta^{\\new} = \\argmax_{\\Theta} \\mathcal{B}(\\Theta,q^{\\new}) \n",
    "$$\n",
    "\n",
    "In general, we can take derivatives, equate them to zero, and solve:\n",
    "$$\n",
    "\\nabla_{\\Theta}  \\mathcal{B}(\\Theta,q^{\\new})  = 0 \n",
    "$$\n",
    "\n",
    "We can show that in our case, the M-step updates are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_c^* &= \\frac{\\sum_i q_i(c) \\xx_i}{\\sum_i q_i(c)} \\\\\n",
    "\\alpha^*_c &= \\frac{\\sum_i q_i(h_i = c)}{N}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
