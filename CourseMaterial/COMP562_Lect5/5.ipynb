{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 â€“ Lecture 5\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\loglik}{\\log\\mathcal{L}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ | }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Scaling -- Feature Scaling\n",
    "\n",
    "* Idea: gradient ascent/descentalgorithm tends to work better if the features are on the **same scale** \n",
    "\n",
    "<img src=\"./Images/Scaling.png\" align=\"center\"/>\n",
    "\n",
    "When contours are skewed then learning steps would take longer to converge due to oscillatory behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Scaling -- Centering\n",
    "\n",
    "**Center** features by removing the mean\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_i &= \\frac{1}{N}\\sum_{k=1}^N x_{i,k}\\\\ \\\\\n",
    "x_{i,j} &= x_{i,j} - \\mu_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This makes each feature's mean equal to 0. Compute the mean first, then subtract it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Scaling -- Standardizing\n",
    "\n",
    "\n",
    "**Standardize** centered features by dividing by the standard deviation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_i &= \\sqrt{ \\frac{1}{N-1}\\sum_j x_{i,j}^2 }\\\\ \\\\\n",
    "x_{i,j}& = \\frac{x_{i,j}}{\\sigma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that standardized features are first centered and then divided by their standard deviation\n",
    "\n",
    "Transform your data to a distribution that has a mean of 0 and a standard deviation of 1 (z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Scaling -- Normalizing\n",
    "\n",
    "Alternatively, **normalize** centered features by dividing by their norm\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_i &= \\sqrt{\\sum_j x_{i,j}^2 }\\\\ \\\\\n",
    "x_{i,j}& = \\frac{x_{i,j}}{r_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that normalized features are first centered and then divided by their norm\n",
    "\n",
    "Normalization transforms your data into a range between 0 and 1 regardless of the data set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Scaling Benefits\n",
    "\n",
    "1. Centering \n",
    "  1. $\\beta_0$ is equal to the mean of the target variable \n",
    "  2. Feature weights $\\beta$ now tell us how much does feature's departure from mean affect the target variable \n",
    "2. Standardization\n",
    "  1. All the features are on the same scale and their effects comparable\n",
    "  2. Interpretation is easier: $\\beta$s tell us how much departure by single standard deviation affects the target variable  \n",
    "3. Normalization\n",
    "  1. Scale of features is the same, regardles of the size of the dataset\n",
    "  2. Hence weights learend on different sized datasets can be compared\n",
    "  3. However, their combination might be problematic -- certainly we don't trust weights learned on few samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification -- Bernoulli View\n",
    "\n",
    "We can model a target variable $y \\in \\{0,1\\}$  using Bernouli distribution\n",
    "\n",
    "$$\n",
    "p(y=1\\given\\theta) = \\theta\n",
    "$$\n",
    "\n",
    "We note that $\\theta$ has to be in range $[0,1]$\n",
    "\n",
    "We cannot directly take weighted combination of features to obtain $\\theta$\n",
    "\n",
    "We need a way to map $\\xx^T\\beta \\in \\mathbb{R}$ to range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some Useful Equalities Involving Sigmoid\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1  + \\exp(-z)}\n",
    "$$\n",
    "\n",
    "Recognize the alternative way to write it:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{\\exp z}{1 + \\exp z} \n",
    "$$\n",
    "\n",
    "Complement is just flip of the sign in the argument\n",
    "\n",
    "$$\n",
    "\\sigma(-z) = 1 - \\sigma(z) \n",
    "$$\n",
    "\n",
    "Log ratio of probability (log odds) \n",
    "\n",
    "$$\n",
    "\\log \\frac{\\sigma(z)}{\\sigma(-z)} = z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Sigmoid to Parameterize Bernoulli\n",
    "\n",
    "$$\n",
    "p(y=1|\\theta) = \\theta\n",
    "$$\n",
    "\n",
    "Sigmoid \"squashes\" the whole real line into range $[0,1]$\n",
    "\n",
    "Hence we can map weighted features into a parameter $\\theta$\n",
    "\n",
    "$$\n",
    "\\theta = \\sigma(\\beta_0 + \\xx^T\\beta) \n",
    "$$\n",
    "\n",
    "and use that $\\theta$  in our Bernoulli\n",
    "\n",
    "$$\n",
    "p(y=1\\given\\theta=\\sigma(\\beta_0 + \\xx^T\\beta) ) = \\sigma(\\beta_0 + \\xx^T\\beta) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression -- Binary Classification\n",
    "\n",
    "In logistic regression we model a binary variable $\\color{red}{y \\in \\{-1,+1\\}}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p({\\color{blue}{y=+1}}\\given\\xx,\\beta_0,\\beta) &= \\sigmoid{{\\color{blue}{+}}(\\beta_0 + \\xx^T\\beta)}\\\\\n",
    "p({\\color{red}{y=-1}}\\given\\xx,\\beta_0,\\beta) &= 1 - \\sigmoid{-(\\beta_0 + \\xx^T\\beta)} = \\sigmoid{{\\color{red}{-}}(\\beta_0 + \\xx^T\\beta)} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is equivalent to\n",
    "\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta) = \n",
    "\\sigmoid{{\\color{green}{y}}(\\beta_0 + \\xx^T\\beta)} = \n",
    "\\frac{1}{1 + \\myexp{-y(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "**<font color='red'> Q: Does above formula work for $y \\in \\{0,1\\}$? </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression -- Decision Boundary\n",
    "\n",
    "$$\n",
    "p(y=1\\given\\xx,\\beta_0,\\beta) = \n",
    "\\sigmoid{(\\beta_0 + \\xx^T\\beta)}= \n",
    "\\frac{1}{1 + \\myexp{-(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1  + \\exp(-z)}\n",
    "$$\n",
    "\n",
    "* Suppose predict \"$y=1$\" if \n",
    "\n",
    "$$\n",
    "p(y=1\\given\\xx,\\beta_0,\\beta) \\geq 0.5 \\rightarrow \\beta_0 + \\xx^T\\beta \\geq 0 \n",
    "$$\n",
    "    \n",
    "* Then predict \"$y=-1$\" if \n",
    "\n",
    "$$\n",
    "p(y=1\\given\\xx,\\beta_0,\\beta) < 0.5 \\rightarrow \\beta_0 + \\xx^T\\beta < 0\n",
    "$$\n",
    "    \n",
    "* Hence, the decision boundary is given by $\\beta_0 + \\xx^T\\beta$ $=$ 0\n",
    "\n",
    "**<font color='red'> Q: What does this decision boundary equation describe? </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression -- Log-Likelihood\n",
    "\n",
    "Probability of a single sample is:\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta) = \\frac{1}{1 + \\myexp{-y(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Likelihood function is:\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta\\given\\yy,\\xx) = \\prod_i \\frac{1}{1 + \\myexp{-y_i(\\beta_0 + \\xx_i^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Log-likelihood function is:\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta\\given\\yy,\\xx) = -\\sum_i \\log\\left\\{1 + \\myexp{-y_i(\\beta_0 + \\xx_i^T\\beta)} \\right\\}\n",
    "$$\n",
    "\n",
    "Follow the same recipe as before to find $\\beta$s that maximize the Log-likelihood function"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
