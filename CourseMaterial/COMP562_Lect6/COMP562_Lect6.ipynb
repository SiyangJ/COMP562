{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 â€“ Lecture 6\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{|}\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression -- Log-Likelihood for $\\pm$ 1 Labels\n",
    "\n",
    "Probability of a single sample is when $y \\in \\{-1,+1\\}$:\n",
    "\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta) = \\frac{1}{1 + \\myexp{-y(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Likelihood function is:\n",
    "\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta\\given\\yy,\\xx) = \\prod_i \\frac{1}{1 + \\myexp{-y_i(\\beta_0 + \\xx_i^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta\\given\\yy,\\xx) = -\\sum_i \\log\\left\\{1 + \\myexp{-y_i(\\beta_0 + \\xx_i^T\\beta)} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression -- Log-Likelihood for $0,1$ Labels\n",
    "\n",
    "Probability of a single sample is when $y \\in \\{0,1\\}$:\n",
    "\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta) = \\frac{\\myexp{y(\\beta_0 + \\xx^T\\beta)}}{1 + \\myexp{(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Likelihood function is:\n",
    "\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta\\given\\yy,\\xx) = \\prod_i \\frac{\\myexp{y_i(\\beta_0 + \\xx_i^T\\beta)}}{1 + \\myexp{(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta\\given\\yy,\\xx) = \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Penalty and Logistic Regression\n",
    "\n",
    "Adding ridge penalty to the logistic regression achieves\n",
    "1. Shrinkage of weights -- weights no longer explode in separable case\n",
    "2. Even splitting between correlated weights\n",
    "\n",
    "Ridge regularized log-likelihood for $\\pm$ 1 labels:\n",
    "\n",
    "$$\n",
    "\\penloglik(\\beta_0,\\beta\\given\\yy,\\xx) = -\\sum_i \\log\\left\\{1 + \\myexp{-y_i(\\beta_0 + \\xx_i^T\\beta)} \\right\\} - \\frac{\\lambda}{2}\\norm{\\beta}^2\n",
    "$$\n",
    "\n",
    "Ridge regularized log-likelihood for $0,1$ labels:\n",
    "$$\n",
    "\\penloglik(\\beta_0,\\beta\\given\\yy,\\xx) =  \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\} - \\frac{\\lambda}{2}\\norm{\\beta}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian View of Penalties\n",
    "\n",
    "We have seen two examples of supervised models\n",
    "1. Linear regression, $p(y|\\xx,\\beta)$ where $y \\in \\mathbb{R}$\n",
    "2. Logistic regression, $p(y|\\xx,\\beta)$ where $y \\in \\{-1,+1\\}$\n",
    "\n",
    "We then utilized log-likelihoods\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta|\\yy,X) = \\sum_i \\log p(y_i|\\xx_i,\\beta)\n",
    "$$\n",
    "\n",
    "and observed that we can add penalties to log-likelihoods\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta|\\yy,X) + \\lambda f(\\beta)\n",
    "$$\n",
    "\n",
    "in order to deal with ill-posedness of the problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian View of Penalties\n",
    "\n",
    "Given a likelihood\n",
    "\n",
    "$$\n",
    "p(\\Data\\given\\theta)\n",
    "$$\n",
    "\n",
    "Bayesian view of models treats each parameter $\\theta$  as just another random variable\n",
    "\n",
    "This random variable has a distribution called **prior** distribution\n",
    "\n",
    "$$\n",
    "p(\\theta)\n",
    "$$\n",
    "\n",
    "Using Bayes rule we can also compute\n",
    "\n",
    "$$\n",
    "\\overbrace{p(\\theta\\given\\Data)}^{\\textrm{posterior}} = \\frac{\n",
    "\\overbrace{p(\\Data\\given \\theta)}^{\\textrm{likelihood}}\n",
    "\\overbrace{p(\\theta)}^{\\textrm{prior}}} {\\underbrace{p(\\Data)}_{\\textrm{evidence}}}\n",
    "$$\n",
    "\n",
    "called **posterior** distribution\n",
    "\n",
    "**Prior** encodes our beliefs **before** seing the data\n",
    "\n",
    "**Posterior** reflects our updated beliefs **after** seeing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian View of Penalties \n",
    "\n",
    "For example we can assume a Gaussian **prior** on $\\beta_i$ to our linear regression model\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_i &\\sim \\Gaussian{0}{\\frac{1}{\\lambda}},& i>0\\\\\n",
    "y &\\sim \\Gaussian{\\beta_0  + \\xx^T\\beta}{\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then posterior probability of the parameter $\\beta_i$:\n",
    "\n",
    "$$\n",
    "p(\\beta \\given \\yy,\\xx) = \\frac{ p(\\yy\\given\\xx,\\beta)p(\\beta) }{p(\\yy\\given\\xx)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian View of Penalties \n",
    "\n",
    "We can now try to find **Maximum-A-Posteriori (MAP)** estimate of $\\theta$ \n",
    "\n",
    "$$\n",
    "\\arg\\max_\\beta p(\\beta \\given \\yy,\\xx) = \\arg\\max_\\beta \\log p(\\yy\\given\\xx,\\beta) + \\log p(\\beta) \n",
    "$$\n",
    "\n",
    "and this is equivalent to\n",
    "\n",
    "$$\n",
    "\\arg\\max_\\beta p(\\beta \\given \\yy,\\xx) = \\arg\\max_\\beta - \\sum_{i=1}^N \\frac{1}{2\\sigma^2}(y_i - \\beta_0 - \\xx_i^T\\beta) - \\sum_{j=1}^p \\frac{\\lambda}{2}\\beta_j^2 +  \\textrm{const}\n",
    "$$\n",
    "\n",
    "Solving ridge regression is equivalent to finding Maximum-A-Posteriori estimate in Bayesian linear regression with Gaussian prior on weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + \\myexp{-z}} = \\frac{\\myexp{z}}{1 + \\myexp{z}}\n",
    "$$\n",
    "\n",
    "Softmax is a generalization of sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma(\\zz)_j = \\frac{\\myexp{z_j}}{\\sum_{c=1}^C \\myexp{z_j}}\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(\\zz)_1 &= \\frac{\\myexp{z_1}}{\\myexp{z_1} + \\myexp{z_2} + \\myexp{z_3}} \\\\\n",
    "\\sigma(\\zz)_2 &= \\frac{\\myexp{z_2}}{\\myexp{z_1} + \\myexp{z_2} + \\myexp{z_3}} \\\\\n",
    "\\sigma(\\zz)_3 &= \\frac{\\myexp{z_3}}{\\myexp{z_1} + \\myexp{z_2} + \\myexp{z_3}} \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiclass Logistic Regression and Softmax\n",
    "\n",
    "We can write out probability of partcular class using softmax\n",
    "\n",
    "$$\n",
    "p(y=c\\given\\xx,\\beta_0,B) = \\boxed{\\frac{ \\myexp{\\beta_{0,c} + \\xx^T\\bbeta_c}}{\\sum_{k=1}^C\\myexp{\\beta_{0,k} + \\xx^T\\bbeta_k}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "B = \\left[\\bbeta_1 \\bbeta_2 ... \\bbeta_C \\right]\n",
    "$$\n",
    "\n",
    "and each $\\bbeta_c$ is a vector of class specific feature weights\n",
    "\n",
    "\n",
    "Note that the $p(y=c\\given\\cdots)$ is a categorical distribution over $C$ possible states, where probabilities of each state are given by softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiclass Logistic Regression -- Log-Likelihood\n",
    "\n",
    "1. There are $N$ samples, each in one of $C$ classes, and $p$ features\n",
    "2. Labels are represented using one-hot vectors $y_i$\n",
    "3. Feature matrix $X$ contains a column of 1s -- corresponding to the bias term\n",
    "4. First row of weight matrix $B$ are bias terms\n",
    "5. $\\bbeta_{k}$ is $k^\\tth$ column of matrix $B$\n",
    "\n",
    "Dimensions:\n",
    "* Feature matrix : $X$ $\\rightarrow$ $N\\times (p+1)$\n",
    "* Label matrix : $Y$ $\\rightarrow$ $N\\times C$\n",
    "* Weight matrix : $B$ $\\rightarrow$ $(p+1)\\times C$\n",
    "\n",
    "Likelihood is \n",
    "\n",
    "$$\n",
    "\\likelihood(B\\given Y,X) = \\underbrace{\\prod_{i=1}^N}_{\\textrm{samples}}\\underbrace{\\prod_{c=1}^C}_{\\textrm{classes}}\\left[\\frac{ \\myexp{\\xx_i^T\\bbeta_c}}{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}\\right]^{y_{i,c}}\n",
    "$$\n",
    "\n",
    "Log-likelihood is\n",
    "$$\n",
    "\\loglik(\\beta_0,B\\given Y,X) = \\sum_{i=1}^N\n",
    "\\sum_{c=1}^C y_{i,c}\\left(\\xx_i^T\\bbeta_c - \\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiclass Logistic Regression -- Regularized Log-Likelihood\n",
    "\n",
    "\n",
    "Ridge regularized log-likelihood\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\penloglik(B\\given Y,X) =& \\sum_{i=1}^N\n",
    "\\sum_{c=1}^C y_{i,c}\\left(\\xx_i^T\\bbeta_c - \\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}} \\right)\\\\ &- \\frac{\\lambda}{2}\\sum_{k=1}^C \\sum_{j=1}^p \\beta_{j,k}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that we keep the last column of $B$ fixed at 0 to get rid of excess parameters\n",
    "\n",
    "These parameters will not contribute to the regularization -- sum of their squares is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-Entropy\n",
    "\n",
    "Frequently you will encounter mentions of cross-entropy. It is negative log likelihood of multiclass logistic\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{crossentropy}(B) &= -\\loglik(B\\given Y,X)\\\\& = -\\sum_{i=1}^N\n",
    "\\sum_{c=1}^C y_{i,c}\\left(\\xx_i^T\\bbeta_c - \\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Ridge regularized cross-entropy \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{crossentropy}(B) =& -\\sum_{i=1}^N \\sum_{c=1}^C y_{i,c}\\left(\\xx_i^T\\bbeta_c - \\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}} \\right) \\\\ \n",
    "&{\\color{red}{+}}\\frac{\\lambda}{2}\\sum_{k=1}^C \\sum_{j=1}^p \\beta_{j,k}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note the sign flip in the regularization"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
