{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 – Lecture 3\n",
    "\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\loglik}{\\log\\mathcal{L}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ | }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding $\\mu^{MLE}$ of Gaussian Distribution\n",
    "\n",
    "We left as an exercise a problem to come up with a maximum likelihood estimate for parameter $\\mu$ of a Gaussian distribution\n",
    "\n",
    "$$\n",
    "p(x\\given\\mu,\\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n",
    "$$\n",
    "\n",
    "So we will do that now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Likelihood function is\n",
    "\n",
    "$$\n",
    "\\likelihood(\\mu,\\sigma^2\\given\\xx) = \\prod_{i=1}^N p(x_i\\given\\mu,\\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}\n",
    "$$\n",
    "\n",
    "Log-likelihood function is\n",
    "\n",
    "$$\n",
    "\\log\\likelihood(\\mu,\\sigma^2\\given\\xx) = \\log \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2} =  \\sum_{i=1}^N \\left[-\\frac{1}{2}\\log{2\\pi\\sigma^2} -\\frac{1}{2\\sigma^2}(x_i-\\mu)^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding $\\mu^{MLE}$ of Gaussian Distribution\n",
    "\n",
    "Our recipe is:\n",
    "\n",
    "1. Take the function you want to maximize: \n",
    "\n",
    "$$\n",
    "f(\\mu) = \\sum_{i=1}^N \\left[-\\frac{1}{2}\\log{2\\pi\\sigma^2}-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2\\right]\n",
    "$$\n",
    "\n",
    "2. Compute its first derivative: $\\frac{\\partial}{\\partial \\mu} f(\\mu)$\n",
    "3. Equate that derivative to zero and solve: $\\frac{\\partial}{\\partial \\mu} f(\\mu) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The first derivative is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} f(\\mu) = \\sum_{i=1}^N \\left[ \\frac{1}{\\sigma^2}(x_i - \\mu)\\right]\n",
    "$$\n",
    "\n",
    "We equate it to zero and solve\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\left[ \\frac{1}{\\sigma^2}(x_i - \\mu)\\right] = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\sum_{i=1}^N x_i}{N} &= \\mu\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding ${\\sigma^{2}}^{MLE}$ of Gaussian Distribution\n",
    "\n",
    "Our recipe is:\n",
    "\n",
    "1. Take the function you want to maximize: \n",
    "\n",
    "$$\n",
    "f(\\sigma^{2}) = \\sum_{i=1}^N \\left[-\\frac{1}{2}\\log{2\\pi\\sigma^2}-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2\\right]\n",
    "$$\n",
    "\n",
    "2. Compute its first derivative: $\\frac{\\partial}{\\partial \\sigma^{2}} f(\\sigma^{2})$\n",
    "3. Equate that derivative to zero and solve: $\\frac{\\partial}{\\partial \\sigma^{2}} f(\\sigma^{2}) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "f(\\sigma^{2}) = \\sum_{i=1}^N \\left[-\\frac{1}{2}\\log{2\\pi\\sigma^2}-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2\\right] = - \\frac{N}{2}\\log{2\\pi} - \\frac{N}{2}\\log{\\sigma^2} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N \\left[(x_i-\\mu)^2\\right]\n",
    "$$\n",
    "\n",
    "The first derivative is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^{2}} f(\\sigma^{2}) = - \\frac{N}{2\\sigma^{2}} -  \\left(\\frac{1}{2} \\sum_{i=1}^N \\left[{(x_i - \\mu)}^{2}\\right]\\right)\\frac{\\partial}{\\partial \\sigma^{2}}\\left(\\frac{1}{\\sigma^{2}}\\right) \\\\\n",
    "= - \\frac{N}{2\\sigma^{2}} -  \\left(\\frac{1}{2} \\sum_{i=1}^N \\left[{(x_i - \\mu)}^{2}\\right]\\right)\\left(-\\frac{1}{{(\\sigma^{2})}^{2}}\\right) = \\frac{1}{2\\sigma^{2}} \\left(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^N \\left[{(x_i - \\mu)}^{2} \\right] - N \\right)\n",
    "$$\n",
    "\n",
    "Which, if we rule out $\\sigma^{2} = 0$, is equal to zero only if \n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\frac{1}{N} \\sum_{i=1}^N \\left[{(x_i - \\mu)}^{2} \\right]\n",
    "$$\n",
    "\n",
    "**<font color='red'> Please Verify both ${\\mu}^{MLE}$ and ${\\sigma^{2}}^{MLE}$ using seconed derivative test </font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "Formally, we would write\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &= \\beta_0 + \\sum_j x_j \\beta_j + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\Gaussian{0}{\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "or more compactly\n",
    "\n",
    "$$\n",
    "y \\given \\xx \\sim \\Gaussian{\\beta_0 + \\sum_j x_j \\beta_j}{ \\sigma^2}\n",
    "$$\n",
    "\n",
    "Notice that the function is linear in the parameters $\\beta=(\\beta_0,\\beta_1,…,\\beta_p)$, not necessarily in terms of the covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "Probability of target variable $y$\n",
    "\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta,\\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(y_i-\\underbrace{(\\beta_0 + \\sum_j x_j \\beta_j)}_{\\textrm{mean of the Gaussian}}\\right)^2\\right\\}\n",
    "$$\n",
    "\n",
    "In the case of the 6th grader's height, we made **the same** prediction for any other 6th grader (58.5 inches)\n",
    "\n",
    "In our COMP 562 grade example, we compute a potentially different mean for every student\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_{\\textrm{COMP410}}*\\textrm{COMP410} + \\beta_{\\textrm{MATH233}}*\\textrm{MATH233} + \\beta_{\\textrm{STOR435}}*\\textrm{STOR435} + \\beta_{\\textrm{beers}}* \\textrm{beers} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Likelihood\n",
    "\n",
    "We start by writing out a likelihood for linear regression is\n",
    "\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \n",
    "\\prod_{i=1}^N p(y\\given\\xx,\\beta_0,\\beta,\\sigma^2) = \n",
    "\\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)^2\\right\\}\n",
    "$$\n",
    "\n",
    "Log-likelihood for linear regression is\n",
    "\n",
    "$$\n",
    "\\log\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)^2\\right] \\\\ = - \\frac{N}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N \\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2 =  - \\frac{N}{2}\\log(2\\pi\\sigma^2) -\\frac{RSS}{2\\sigma^2} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will refer to expression $y_i-(\\beta_0 + \\sum_j x_j \\beta_j)$ as **residual**, and hence **RSS** stands for **residual sum of squares** or **sum of squared errors** and is defined by \n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^N \\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2\n",
    "$$\n",
    "\n",
    "And RSS/N is called the **mean squared error** or **MSE**\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^N \\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2\n",
    "$$\n",
    "\n",
    "Hence, maximizing log-likelihood is equivalent to minimizing RSS or MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to see this is to consider a very simplified version of Taylor's theorem\n",
    "\n",
    "**Theorem.** Given a function $f(\\cdot)$ which is smooth at $x$\n",
    "\n",
    "$$\n",
    "f(x + d) = f(x) + f'(x)d + O(d^2)\n",
    "$$\n",
    "\n",
    "In words, close to $x$ function $f(\\cdot)$ is very close to being a linear function of $d$\n",
    "\n",
    "$$\n",
    "f(x + \\color{blue}{d}) = f(x) + f'(x)\\color{blue}{d}\n",
    "$$\n",
    "\n",
    "Slope of the best linear approximation is $f'(x)$, i.e.,$\\hspace{0.5em}$$f'(x)$ tells us in which direction function grows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Gradient Ascent\\Descent: Choose initial ${\\mathbf{\\theta}^{(0)}} \\in \\mathbb{R}^{n}$, repeat:\n",
    "$$  \\;\n",
    "\\begin{aligned}\n",
    "{\\mathbf{\\theta}^{(k)}} = {\\mathbf{\\theta}^{(k-1)}} \\pm t_{k}.\\nabla f({\\mathbf{\\theta}^{(k-1)}}), k =1,2,3,\\ldots\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $t_{k}$ is the step size (learning rate) at step $k$\n",
    "\n",
    "* Stop at some point using a stopping criteria (depend on the problem we are solving), for example:\n",
    " * Maximum number of iterations reached\n",
    " * $| f({\\mathbf{\\theta}^{(k)}}) − f({\\mathbf{\\theta}^{(k-1)}}) | < \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Use Line search Strategy\n",
    " * At each iteration, do the best you can along the direction of the gradient,\n",
    " \n",
    " $$\n",
    "\\begin{aligned}\n",
    "t = \\mathop{\\textrm{argmax}}_{s \\geq 0} f(\\mathbf{\\theta} + s.\\nabla f({\\mathbf{\\theta}}))\n",
    "\\end{aligned}\n",
    " $$\n",
    "  \n",
    " * Usually, it is not possible to do this minimization exactly, and approximation methods are used\n",
    " * Backtracking Line Search: \n",
    "     * Choose an initial learning rate ($t_{k} = t_{init})$, and update your parameters ${\\mathbf{\\theta}^{(k)}} = {\\mathbf{\\theta}^{(k-1)}} \\pm t_{k}.\\nabla f({\\mathbf{\\theta}^{(k-1)}})$ \n",
    "     * Reduce learning rate $t_{k} = \\alpha . t_{init}$, where  $0< \\alpha <1 $\n",
    "     * Repeat by reducing $\\alpha$ till you see an improvmnet in $f({\\mathbf{\\theta}^{(k)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Likelihood\n",
    "\n",
    "We start by writing out a likelihood for linear regression is\n",
    "\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \n",
    "\\prod_{i=1}^N p(y\\given\\xx,\\beta_0,\\beta,\\sigma^2) = \n",
    "\\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)^2\\right\\}\n",
    "$$\n",
    "\n",
    "Log-likelihood for linear regression is\n",
    "\n",
    "$$\n",
    "\\log\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)^2\\right].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Gradient of Log-Likelihood\n",
    "\n",
    "Partial derivatives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\beta_0} \\log\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy)  &= \\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)(-1)\\\\\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\log\\likelihood(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy)  &= \\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)(-x_k)&,k\\in\\{1,\\dots,p\\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence gradient (with respect to $\\beta$s)\n",
    "$$\n",
    "\\nabla \\loglik(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \\left[\\begin{array}{c} \n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)(-1) \\\\\n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_1 + \\sum_j x_j \\beta_j)\\right)(-x_1) \\\\\n",
    "\\vdots\\\\\n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_j \\beta_j)\\right)(-x_p)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
