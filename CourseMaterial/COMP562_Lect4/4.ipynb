{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 â€“ Lecture 4\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\loglik}{\\log\\mathcal{L}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{|}\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Matrix Form\n",
    "\n",
    "A general multiple-regression model can be written as\n",
    "\n",
    "$$\n",
    "y \\given \\xx = \\beta_0 + \\sum_j x_j \\beta_j + \\epsilon, \\ \\ \\ \\ \\epsilon \\sim \\Gaussian{}{0, \\sigma^2}\n",
    "$$\n",
    "Which is equivalent to\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots +  \\beta_p x_{ip} + \\epsilon_i \\ \\ \\ \\ for \\ i=1,\\ldots,N\n",
    "$$\n",
    "\n",
    "In matrix form, we can rewrite this model as \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} { y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N}\\end{array}\\right]_{\\ N \\times 1} = \\left[\\begin{array}{ccccc} 1  & x_{11} & x_{12} & \\ldots & x_{1p} \\\\ 1  & x_{21} & x_{22} & \\ldots & x_{2p} \\\\ \\vdots  & \\vdots & \\vdots & \\ldots & \\vdots \\\\ 1  & x_{N1} & x_{N2} & \\ldots & x_{Np} \\end{array}\\right]_{\\ N \\times p+1} \\left[\\begin{array}{c} \\ { \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_P}\\end{array}\\right]_{\\ p+1 \\times 1} \\ + \\left[\\begin{array}{c} \\ { \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N}\\end{array}\\right]_{\\ N \\times 1}\n",
    "$$\n",
    "\n",
    "This can be rewritten more simply as:\n",
    "$$\n",
    "\\yy = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Closed Form Solution for $\\beta$\n",
    "\n",
    "Remember that maximizing log-likelihood is equivalent to minimizing **RSS** or **MSE**\n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^N \\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2 = \\sum_{i=1}^N \\left(e_i\\right)^2 = \\mathbf{e_i}^{T} \\mathbf{e_i} = \\left[\\begin{array}{cccc} e_1 & e_2 & \\dots & e_N \\end{array}\\right]_{1 \\times N} \\ \\left[\\begin{array}{c} { e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_N}\\end{array}\\right]_{\\ N \\times 1} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "RSS &= (\\yy - \\mathbf{X} \\mathbf{\\beta})^{T} (\\yy- \\mathbf{X} \\mathbf{\\beta}) \\\\ &= (\\yy^{T}  - \\mathbf{\\beta}^{T} \\mathbf{X}^{T}) (\\yy - \\mathbf{X} \\mathbf{\\beta})  \\\\ &= \\yy^{T} \\yy  -  \\beta^{T} \\mathbf{X}^{T} \\yy - \\yy^{T} \\mathbf{X} \\mathbf{\\beta} + \\beta^{T} \\mathbf{X}^{T} \\mathbf{X} \\beta \\\\ &= \\yy \\yy^{T} -  2 \\beta^{T} \\mathbf{X}^{T} \\yy + \\beta^{T} \\mathbf{X}^{T} \\mathbf{X} \\beta \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where this development uses the fact that the transpose of a scalar is the scalar i.e. $\\beta^{T} \\mathbf{X}^{T} \\yy = \\yy^{T} \\mathbf{X} \\mathbf{\\beta}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To find the $\\mathbf{\\beta}$ that minimizes $RSS$, we solve the following equation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta \\ RSS = -2 \\mathbf{X}^{T} \\yy + 2 \\mathbf{X}^{T} \\mathbf{X} \\beta = 0\n",
    "$$\n",
    "\n",
    "The corresponding solution to this linear system of equations is called the **ordinary least squares** or **OLS** solution\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\beta^{\\textrm{OLS}} = \\beta^{\\textrm{MLE}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\yy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression -- Closed Form Solution for $\\sigma^2$\n",
    "\n",
    "Recall log-likelihoood function\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta|\\yy,\\xx) = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Which can be written in matrix form\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta\\given\\yy,\\xx) = -\\frac{N}{2}\\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}(\\yy -  \\mathbf{X} \\beta)^T(\\yy -  \\mathbf{X} \\beta) \n",
    "$$\n",
    "\n",
    "Taking derivative and equating it to zero yields\n",
    "\n",
    "$$\n",
    "(\\sigma^{2})^\\MLE = \\frac{1}{N} (\\yy -  \\mathbf{X} \\beta^\\MLE)^T(\\yy -  \\mathbf{X} \\beta^\\MLE)  = \\frac{1}{N} \\sum_{i=1}^N \\left(y_i - (\\beta_0^\\MLE + \\sum_j x_{i,j} \\beta_j^\\MLE)\\right)^2\n",
    "$$\n",
    "\n",
    "**<font color='red'> Please verify $(\\sigma^{2})^\\MLE$ at home </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Overfitting**: Model every minor variation in the input using highly flexible complex models \n",
    "    * High variance and low bias\n",
    "\n",
    "* **Underfitting**: Simple model that is unable to capture the true relationships in given data\n",
    "    * Low variance and high bias\n",
    "\n",
    "* **Model Selection**: Picking the right model from a variety of models of different complexity\n",
    "\n",
    "**<font color='red'> Q: Which model overfits/underfits the data? </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "The **mean quared errors** or **MSE** may be decomposed into **bias** and **variance** components:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\mathop{\\mathbb{E}}(y - \\hat{y})^2}_{\\mathbf{MSE}} = \\underbrace{(\\mathop{\\mathbb{E}}(\\hat{y}) - y)^2}_{\\mathbf{Bias^2}} + \\underbrace{\\mathop{\\mathbb{E}} \\left[ (\\hat{y} - \\mathop{\\mathbb{E}}(\\hat{y}))^2 \\right]}_{\\mathbf{Variance}} + \\underbrace{\\sigma^2_{e}}_{\\mathbf{Irreducible \\ Error}}\n",
    "$$\n",
    "\n",
    "<img src=\"./Images/biasvariance.png\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ill-Posed Problems\n",
    "\n",
    "**<font color='red'> Q: What happens if you are solving a linear system $Ax = y$ and there are more unknowns than equations? </font>**\n",
    "\n",
    "In our setting -- $N$ samples, $P$ features --  linear regresion is ill-posed if $P>N$\n",
    "\n",
    "Another example of ill-posed linear regression problem arises when we have two copies of the same predictors \n",
    "\n",
    "This is a problem even if $P<N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Adding that penalty to linear regression log-likelihood yields **ridge regresion**\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta|\\yy,\\xx) = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)^2\\right] - \\underbrace{\\frac{\\lambda}{2} \\sum_{j} \\beta_{j}^2}_{\\textrm{ridge penalty}}\n",
    "$$\n",
    "\n",
    "All those sums can get cumbersome, so we will use norms\n",
    "1. $\\ell_2$ norm $\\norm{\\xx} = \\sqrt{\\sum_{i} x_i^2}$\n",
    "2. $\\ell_1$ norm $\\norm{\\xx}_1 = \\sum_{i} \\left|x_i\\right|$\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta\\given\\yy,\\xx) =  -\\frac{1}{2\\sigma^2}\\norm{\\yy - \\mathbf{X} \\mathbf{\\beta}}^2  \\underbrace{-\\frac{\\lambda}{2}\\norm{\\beta}^2}_{\\textrm{ridge penalty}}+ \\textrm{const.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression -- Computing Gradients\n",
    "\n",
    "$$\n",
    "\\loglik(\\beta\\given\\yy,\\xx) =  -\\frac{1}{2\\sigma^2}\\norm{\\yy - \\mathbf{X} \\mathbf{\\beta}}^2  \\underbrace{-\\frac{\\lambda}{2}\\norm{\\beta}^2}_{\\textrm{ridge penalty}}+ \\textrm{const.}\n",
    "$$\n",
    "\n",
    "Computing the gradient and setting it to zero\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta \\loglik(\\beta\\given\\yy,\\xx) = \\frac{1}{\\sigma^2}X^T(\\yy - \\mathbf{X} \\mathbf{\\beta}) - \\lambda\\beta = 0\n",
    "$$\n",
    "\n",
    "yields\n",
    "\n",
    "$$\n",
    "\\beta^{\\MLE} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\sigma^2 \\mathbf{I}_N)^{-1}(\\mathbf{X}^T\\yy)\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{I}_N$ is the identity matrix of size $N$\n",
    "\n",
    "Contrast this to closed form solution of linear regression\n",
    "\n",
    "$$\n",
    "\\beta^{\\textrm{MLE}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\yy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression -- Computing Gradients\n",
    "\n",
    "The bias/intercept coefficient $\\beta_0$ is typically not regularized in a linear regression\n",
    "\n",
    "A regularized $\\beta_0$ (shrinked) may us from prevent finding the correct relationship\n",
    "\n",
    "$$\n",
    "\\nabla \\loglik(\\beta_0,\\beta,\\sigma^2\\given\\xx,\\yy) = \\left[\\begin{array}{c} \n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)(-1) \\\\\n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_1 + \\sum_j x_{i,j} \\beta_j)\\right)(-x_{i,1}) - \\lambda \\beta_1 \\\\\n",
    "\\vdots\\\\\n",
    "\\sum_{i=1}^N -\\frac{1}{\\sigma^2}\\left(y_i-(\\beta_0 + \\sum_j x_{i,j} \\beta_j)\\right)(-x_{i,p}) - \\lambda \\beta_p\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note that  $\\beta_0$ is **not** regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Remember our closed form solution for ridge regression\n",
    "\n",
    "$$\n",
    "\\beta^{\\MLE} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\sigma^2 \\mathbf{I}_N)^{-1}(\\mathbf{X}^T\\yy)\n",
    "$$\n",
    "\n",
    "Updating our closed form solution without regularizing $\\beta_0$ will yeild\n",
    "\n",
    "$$\n",
    "\\beta^{\\MLE} = \\left(\\mathbf{X}^T\\mathbf{X} + \\lambda\\sigma^2 \\left[\\begin{array}{ccccc} 0  & 0 & 0 & \\ldots & 0 \\\\ 0  & 1 & 0 & \\ldots & 0 \\\\ \\vdots  & \\vdots & \\vdots & \\ldots & \\vdots \\\\ 0  & 0 & 0 & \\ldots & 1 \\end{array}\\right]_{\\ N \\times N}\\right)^{-1}(\\mathbf{X}^T\\yy)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
