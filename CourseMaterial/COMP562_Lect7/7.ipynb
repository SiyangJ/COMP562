{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 â€“ Lecture 7\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ | }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradients of Multiclass Logistic Regression log-likelihood\n",
    "\n",
    "We will work this out in a pedestrian fashion and then obtain a compact expression:\n",
    "\n",
    "$$\n",
    "\\loglik(B)=\\sum_{i=1}^N \\sum_{c=1}^C y_{i,c}\\left(\\underbrace{\\xx_i^T\\bbeta_c}_{\\textrm{involves only $\\beta_c$}} - \\underbrace{\\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}}_{\\textrm{involves all columns of $B$}} \\right) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\loglik(B)&=\\sum_{i=1}^N \\sum_{c=1}^C y_{i,c}\\left(\\xx_i^T\\bbeta_c\\right) -\\sum_{i=1}^N\\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we need to compute log-likelihood partial derivative with respect to $\\beta_{j,c}$ ( $\\beta$ associated with feature $j$ and class $c$ ) \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\beta_{j,c}} \\loglik(B) &= \\sum_{i=1}^N y_{i,c}x_{i,j} -\\sum_{i=1}^N \\frac{\\partial}{\\partial \\beta_{j,c}}\\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "On board we will work out \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{j,c}}\\mylog{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}} = \\boxed{\\frac{\\myexp{\\xx_i^T\\bbeta_c}}{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}}x_{i,j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\beta_{j,c}} \\loglik(B) &= \\sum_{i=1}^N y_{i,c}x_{i,j} -\\sum_{i=1}^N \\frac{\\myexp{\\xx_i^T\\bbeta_c}}{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}x_{i,j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For compactness, model's probability of class $c$ for sample $i$ will be denoted $\\mu_{i,c}$\n",
    "\n",
    "$$\n",
    "\\mu_{i,c} = \\frac{\\myexp{\\xx_i^T\\bbeta_c}}{\\sum_{k=1}^C\\myexp{\\xx_i^T\\bbeta_k}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\beta_{j,c}} \\loglik(B) = \\sum_{i=1}^N y_{i,c}x_{i,j}\n",
    "-\\sum_{i=1}^N \\mu_{i,c}x_{i,j} = \\sum_{i=1}^N x_{i,j}\\underbrace{(y_{i,c} - \\mu_{i,c})}_{\\textrm{residual}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\beta_{j,c}} \\loglik(B) = \\sum_{i=1}^N \\underbrace{x_{i,j}}_{\\textrm{feature $j$}}\\underbrace{(y_{i,c} - \\mu_{i,c})}_{\\ \\ \\ \\textrm{ residual in} \\\\ \\textrm{predicting class $c$}} \n",
    "$$\n",
    "\n",
    "In words, partial derivative of log-likelihood with respect to $j^\\tth$ feature's weight for class $c$ is inner product between the feature and disagreement between prediction and the true label\n",
    "\n",
    "Gradient of log likelihood with respect to a column of $B$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta_c} \\loglik(B) = \\sum_{i=1}^N (y_{i,c} - \\mu_{i,c})\\xx_i\n",
    "$$\n",
    "\n",
    "Gradient of ridge regularized log-likelihood with respect to a column of $B$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta_c} \\penloglik(B) = \\sum_{i=1}^N (y_{i,c} - \\mu_{i,c})\\xx_i - \\lambda\\left[\\begin{aligned}0\\\\\\ones_p\\end{aligned}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BigData and Stochastic Gradient\n",
    "\n",
    "Once the number of samples becomes large iterating over all of them has diminishing returns\n",
    "\n",
    "Stochastic gradient methods compute gradients using a portion of data called **mini-batches**\n",
    "\n",
    "An extreme example of this is **online learning** -- data is streamed one sample at a time\n",
    "\n",
    "Note: data is usually **randomaly permuted (shuffled)** before each iteration when using stochastic gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Updating parameters based on a small set of data if not guaranteed to monotonically improve log-likelihood\n",
    "\n",
    "Hence, step-size cannot be chosen using line-search \n",
    "\n",
    "Instead, step sizes for each iteration $k$ are computed \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t^{(k)} &= \\left(t^{(k-1)}\\right)^{1-\\epsilon} & \\epsilon \\in [0,1]\\\\\n",
    "t^{(k)} &= \\frac{1}{\\tau + k} & \\tau > 0 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and lead to diminishing step-size (learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Gaussian Distribution -- Independent Case\n",
    "\n",
    "Since $z_1$ and $z_2$ are independent we can write out the joint \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(z_1,z_2) &= p(z_1)p(z_2) \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma_1^2} }\n",
    "   \\frac{1}{\\sqrt{2\\pi\\sigma_2^2} }\\myexp{-\\frac{1}{2\\sigma_1^2}z_1^2}\\myexp{-\\frac{1}{2\\sigma_1^2}z_2^2}\\\\\n",
    "&= \\frac{1}{\\sqrt{(2\\pi)^2\\sigma_1^2\\sigma_2^2}}\\myexp{ -\\frac{1}{2\\sigma_1^2}z_1^2  -\\frac{1}{2\\sigma_2^2}z_2^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fact, for multiple independent Gaussian random variables $z_1,...,z_p$ the joint is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(z_1,\\dots,z_p) &= \\prod_i p(z_i) \\\\\n",
    "&= (2\\pi)^{-p/2}\\left(\\prod_{i=1}^p \\sigma_i^2\\right)^{-1/2}\\myexp{-\\sum_{i=1}^p \\frac{1}{2\\sigma_i^2}z_i^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Gaussian Distribution -- Dependent Case\n",
    "\n",
    "Suppose we have $p$ standard random variables  (0 mean, unit variance)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i \\sim& \\Gaussian{0}{1},&  i=1,\\dots p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we are given a vector $\\mmu$ of length $p$ and a full-rank matrix $A$ of size $p \\times p$\n",
    "\n",
    "What does distribution of $\\xx = A\\zz + \\mmu$ look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Gaussian Distribution -- Dependent Case\n",
    "\n",
    "Suppose we have $n$ standard random variables  (0 mean, unit variance)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i \\sim& \\Gaussian{0}{1},&  i=1,\\dots p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we are given a vector $\\mmu$ of length $n$ and a full-rank matrix $A$ of size $p \\times p$\n",
    "\n",
    "Distribution of $\\xx = A\\zz + \\mu$ is\n",
    "$$\n",
    "p(\\xx) = \\left(2\\pi\\right)^{-\\frac{p}{2}}(\\det{\\Sigma})^{-\\frac{1}{2}}\\myexp{\\frac{1}{2}(\\xx - \\mmu)^T\\Sigma^{-1}(\\xx-\\mmu)}\n",
    "$$\n",
    "where $\\Sigma = AA^T$.\n",
    "\n",
    "* $\\mu$ is **mean** of the Gaussian\n",
    "* $\\Sigma$ is **covariance** matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimates of Mean and Covariance\n",
    "\n",
    "Given data $\\{\\xx_i \\in \\reals^n|i=1,\\dots,N\\}$ maximum likelihood estimates (MLE) of mean and covariance are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mmu^{\\MLE} &= \\frac{1}{N}\\sum_{i=1}^N \\xx_i\\\\\n",
    "\\Sigma^{\\MLE} & = \\frac{1}{N}\\sum_{i=1}^N \\underbrace{\\left(\\xx_i - \\mmu^{\\MLE}\\right)\\left(\\xx_i - \\mmu^{\\MLE}\\right)^T}_{\\textrm{ a matrix of size $p \\times p$}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Dimensionality\n",
    "* $\\mmu^{\\MLE}$ is of same dimension as a single data point $p \\times 1$.\n",
    "* $\\Sigma^{\\MLE}$ is a matrix of size $p \\times p$ \n",
    "\n",
    "Note that $\\xx\\xx^T$ and $\\xx^T\\xx$ are not the same, former is a matrix, latter is a scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative vs Discriminative Approaches to Classification\n",
    "\n",
    "Thus far, we posed classification problems in terms of learning conditional probabilities of labels $y$ given features $\\xx$\n",
    "\n",
    "$$\n",
    "p(y\\given \\xx,\\theta)\n",
    "$$\n",
    "\n",
    "and we optimized **conditional** log-likelihood\n",
    "\n",
    "$$\n",
    "\\loglik(\\theta|\\yy,X) = \\sum_i \\log \\underbrace{p(y_i \\given \\xx_i,\\theta)}_{\\textrm{conditional probability}}\n",
    "$$\n",
    "\n",
    "We did not care about how features $\\xx$ were distributed\n",
    "\n",
    "Our aim was to increase probability of labels given features\n",
    "\n",
    "This approach to learning is called **discriminative** -- we learn to discriminate between different classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative vs Discriminative Approaches to Classification\n",
    "\n",
    "Generative models describe all of the data\n",
    "\n",
    "$$\n",
    "p(y,\\xx\\given \\theta)\n",
    "$$\n",
    "\n",
    "and optimize **joint** log-likelihood\n",
    "\n",
    "$$\n",
    "\\loglik(\\theta\\given\\yy,X) = \\sum_i \\log \\underbrace{p(y_i, \\xx_i\\given\\theta)}_{\\textrm{joint probability}}\n",
    " = \\sum_i \\left[\\log \\underbrace{p(y_i \\given \\xx_i,\\theta)}_{\\textrm{conditional probability}} + {\\color{red}{\\log \\underbrace{p(\\xx_i\\given\\theta)}_{\\textrm{marginal probability}}}}\\right]$$\n",
    " \n",
    "In this setting, the log-likelihood can be improved by:\n",
    "1. Increasing conditional probability of labels given features $p(y_i\\given\\xx_i,\\theta)$\n",
    "2. Increasing probability of features $p(\\xx_i\\given\\theta)$\n",
    "\n",
    "However, given such a model we can describe how the data as a whole -- both features and labels -- were generated\n",
    "\n",
    "This approach to learning is called **generative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models for Classification \n",
    "\n",
    "There are two ways to factorize joint probability of labels and features\n",
    "\n",
    "$$\n",
    "p(y,\\xx\\given\\theta) = p(y\\given\\xx,\\theta)p(\\xx\\given\\theta) =  p(\\xx\\given y,\\theta)p(y\\given\\theta) \n",
    "$$\n",
    "\n",
    "The second one given us a simple process to *GENERATE* data:\n",
    "\n",
    "1. First select label according $p(y\\given\\theta)$, say it was $c$\n",
    "2. Now generate features $p(\\xx\\given y=c,\\theta)$\n",
    "\n",
    "Once we have such a model, to *CLASSIFY* new data, we can obtain the conditional probability $p(y\\given\\xx)$ using Bayes rule\n",
    "\n",
    "$$\n",
    "p(y=c\\given \\xx) = \\frac{p(y=c,\\xx\\given\\theta)}{p(\\xx\\given\\theta)} = \\frac{p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta)}{\\sum_k p(y=k\\given\\theta)p(\\xx\\given y=k,\\theta)}\n",
    "$$\n",
    "\n",
    "and we can predict label for a new feature vector $\\xx$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models for Classification -- Prediction\n",
    "\n",
    "If we are only interested in predicting the most likely class -- rather than computing probabilities -- we can simplify math a bit by observing\n",
    "\n",
    "$$\n",
    "p(y=c\\given \\xx) = \\frac{p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta)}{\\underbrace{\\sum_k p(y=k\\given\\theta)p(\\xx\\given y=k,\\theta)}_{\\textrm{does not depend on c}}}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "p(y=c\\given \\xx) \\propto p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathop{\\textrm{argmax}}_c p(y=c\\given \\xx) &= \\mathop{\\textrm{argmax}}_c p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta) \\\\\n",
    "&=\n",
    "\\mathop{\\textrm{argmax}}_c \\log  p(y=c\\given\\theta) + \\log p(\\xx\\given y=c,\\theta)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
